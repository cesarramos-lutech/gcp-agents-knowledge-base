# CLAUDE.md â€” CogniBI Playbook

> Este fichero es el punto de arranque para cualquier sesiÃ³n.
> LÃ©elo antes de empezar. ActualÃ­zalo al terminar cada sesiÃ³n.

---

## â–¶ CÃ³mo arrancar una sesiÃ³n

```
"Lee el CLAUDE.md en gcp-agents-knowledge-base/cognibi/ y arranca la sesiÃ³n S[N] de CogniBI"
```

Claude Code leerÃ¡ este fichero y empezarÃ¡ con el contexto completo, sin necesidad de repetir nada.

---

## âš ï¸ REGLAS IMPERATIVAS DE SESIÃ“N

### 1. Las sesiones requieren el input de CÃ©sar â€” NO son autÃ³nomas

Claude Code **NO debe ejecutar tareas de manera autÃ³noma**. Cada paso importante dentro de una sesiÃ³n requiere confirmaciÃ³n explÃ­cita del usuario antes de proceder.

**Protocolo obligatorio:**
- Al arrancar: presentar el plan de la sesiÃ³n y preguntar si CÃ©sar quiere empezar, modificar el alcance, o priorizar algo diferente
- Ante cada tarea o entrega: usar `AskUserQuestion` para confirmar direcciÃ³n antes de generar output
- Ante decisiones de diseÃ±o: presentar opciones y esperar elecciÃ³n, no decidir unilateralmente
- Ante dudas: preguntar, no asumir

**Ejemplo correcto:**
> "La sesiÃ³n S2 tiene estas 5 tareas. Â¿Empezamos por el anÃ¡lisis de data-science-agent o prefieres otro orden?"

**Ejemplo incorrecto:**
> [Claude ejecuta las 5 tareas sin preguntar y presenta el documento terminado]

### 2. Las sesiones del calendario son flexibles

El calendario es una guÃ­a, no un contrato. Las sesiones pueden:
- **Moverse** a otro dÃ­a segÃºn la agenda de CÃ©sar
- **Dividirse** en sub-sesiones si una tarea es mÃ¡s grande de lo esperado
- **Fusionarse** si el trabajo va mÃ¡s rÃ¡pido
- **Reordenarse** si cambian las prioridades
- **AÃ±adir sesiones nuevas** para tareas no previstas inicialmente

Cuando esto ocurra: actualizar este CLAUDE.md y, si aplica, los eventos del calendario.

### 3. IMPERATIVO: Guardar la foto al terminar cada sesiÃ³n

**Al finalizar cualquier sesiÃ³n** (o si se interrumpe), Claude Code DEBE actualizar este fichero con:
- El estado actualizado de la tabla de sesiones (âœ… / â³ / ğŸš§ en curso)
- Una nota en "Notas por sesiÃ³n" con: quÃ© se hizo, quÃ© quedÃ³ pendiente, decisiones tomadas
- Las "Decisiones abiertas" actualizadas
- La sesiÃ³n SIGUIENTE marcada como **SIGUIENTE**

Esto garantiza que en cualquier sesiÃ³n futura, partimos con contexto completo aunque no haya memoria de conversaciones anteriores.

---

## Estado actual del plan de trabajo

| SesiÃ³n | Nombre | Estado | Fecha |
|--------|--------|--------|-------|
| S1 | Product Brief | âœ… COMPLETADO | 24 Feb 2026 |
| S1.5 | Competitive Landscape | â³ **SIGUIENTE** | 26 Feb 2026 |
| S2 | Asset Analysis | â³ pendiente | 2 Mar 2026 |
| S3 | Agent Deliverable Spec | â³ pendiente | 4 Mar 2026 |
| S4 | MetodologÃ­a | â³ pendiente | 6 Mar 2026 |
| S5 | Arquitectura Template | â³ pendiente | 9 Mar 2026 |
| S6 | Build: Template v0.1 | â³ pendiente | 11 Mar 2026 |
| S7 | Build: Client Onboarding | â³ pendiente | 13 Mar 2026 |
| S8 | Build: Claude Code Automation | â³ pendiente | 16 Mar 2026 |
| S9 | Demo + Pitch | â³ pendiente | 18 Mar 2026 |
| S10 | Review + Package | â³ pendiente | 20 Mar 2026 |
| S11 | Smoke Test E2E (GA4 + Ads) | â³ pendiente | 23 Mar 2026 |
| S12 | Governance & Ops Layer | â³ pendiente | 25 Mar 2026 |
| S13 | Market Validation & Battle Cards | â³ pendiente | 27 Mar 2026 |

**Ãšltima sesiÃ³n:** S1 â€” Product Brief (24 Feb 2026)
**PrÃ³xima sesiÃ³n:** S1.5 â€” Competitive Landscape (26 Feb 2026)

---

## Contexto del producto

**CogniBI** = acelerador de consultorÃ­a de BI para entregar **agentes conversacionales sobre datos** en empresas medianas.

### El problema
- **Cliente final:** datos existen pero no son accesibles â€” cualquier pregunta nueva requiere esperar al analista
- **Consultor:** 70% del proyecto es trabajo repetitivo que CogniBI + Claude Code automatiza

### El deliverable al cliente
Agente conversacional deployado en GCP con:
- NL2SQL (pregunta en espaÃ±ol â†’ SQL â†’ resultado)
- Visualizaciones automÃ¡ticas (Vega-Lite)
- UI web (Streamlit o React)
- API (FastAPI + WebSocket)
- Contexto de negocio embebido (KPIs, glosario, relaciones de tablas)
- Deployment: Cloud Run + Firestore (sesiones) + GCS (artefactos)

### ICP (cliente ideal)
Empresa mediana B2B, 50â€“500 empleados, sector SaaS/retail/fintech/logÃ­stica, datos ya en BigQuery o Salesforce, sin equipo de IA propio, presupuesto â‚¬15kâ€“â‚¬50k.

### Modelo de negocio
- Fee implementaciÃ³n: â‚¬15kâ€“â‚¬40k por proyecto
- Retainer mensual: â‚¬1kâ€“â‚¬3k/mes, que incluye:
  - **Operaciones:** monitorizaciÃ³n interna del agente (system health, SQL correctness, costes de inferencia)
  - **Business Value Report:** informe mensual al cliente con mÃ©tricas de ROI (time-to-insight, adoption, % queries exitosas)
  - **Mantenimiento evolutivo:** actualizaciones de schema, glosario, upgrades de modelo

### Decisiones abiertas (confirmar con CÃ©sar)
- [ ] Â¿El nombre "CogniBI" es definitivo?
- [ ] Â¿Solo GCP/Gemini o hay que soportar otros LLMs?
- [ ] Â¿UI Streamlit (MVP rÃ¡pido) o React (look enterprise)?

---

## Base tÃ©cnica existente

### `data-science-agent` â€” base principal del template
**Path:** `gcp-data-agents/data-science-agent/`
- ADK v1.14+, Gemini 2.5 Pro
- BigQuery (NL2SQL con CHASE-SQL) + AlloyDB (MCP Toolbox) + Python analytics + BQML
- Root agent â†’ 4 sub-agentes (BQ, AlloyDB, Analytics, BQML)
- Config dataset: `flights_dataset_config.json` â†’ define datasets + relaciones FK
- Deployment: ADK local / Vertex AI Agent Engine / Cloud Run
- **Generalizar:** eliminar "flights/demo", parametrizar datasets por cliente

### `crm-data-agent-cesar` â€” patrÃ³n BI para usuario de negocio
**Path:** `gcp-data-agents/crm-data-agent-cesar/`
- ADK v1.3.x, Gemini 2.5 Pro (root/BA/DE/BI) + Gemini 2.0 Flash (evaluador visual)
- BigQuery + Salesforce Data Cloud + Vega-Lite + Streamlit + FastAPI
- Root agent â†’ 3 tools: CRM Business Analyst (LlmAgent), Data Engineer (FunctionTool), BI Engineer (FunctionTool)
- Loop de calidad de charts con modelo vision (hasta 5 iteraciones)
- Self-correction SQL hasta 32 intentos con re-inyecciÃ³n de schema
- Metadata: `sfdc_metadata.json` (~470KB) en contexto
- **Generalizar:** reemplazar sfdc_metadata por formato genÃ©rico, parametrizar prompts por dominio

### PatrÃ³n arquitectÃ³nico comÃºn (el "CogniBI pattern")
```
Root Agent (orquestador)
  â”œâ”€â”€ Business Analyst (LlmAgent) â†’ interpreta la pregunta de negocio
  â”œâ”€â”€ Data Engineer (FunctionTool) â†’ NL2SQL + self-correction loop
  â””â”€â”€ BI Engineer (FunctionTool) â†’ ejecuta SQL + genera viz + evalÃºa calidad
```

### Otros repos de referencia
- `ca-demos-and-tools-CESAR/` â€” ejemplos ADK pÃºblicos (streaming, Prism ORM, React frontend, CA API)
- `ca-api-quickstarts-CESAR/` â€” quickstarts ADK con Streamlit

---

## Herramientas disponibles en Claude Code para este proyecto

### Skill: `google-adk`

Claude Code tiene una **skill especializada en Google ADK** que actÃºa como experto en el framework.

**CÃ³mo invocarla:**
```
/google-adk
```

**QuÃ© puede hacer:**
- Responder preguntas sobre arquitectura de agentes ADK (LlmAgent, FunctionTool, AgentTool, etc.)
- Revisar cÃ³digo ADK y detectar antipatrones
- Comparar implementaciones contra las best practices oficiales
- Guiar decisiones de diseÃ±o: cuÃ¡ndo usar sub-agentes vs tools, cÃ³mo gestionar estado, cÃ³mo hacer deploy en Vertex AI Agent Engine vs Cloud Run
- Contrastar lo que tenemos construido vs lo que el framework recomienda

**CuÃ¡ndo usarla:** siempre que haya una decisiÃ³n de arquitectura ADK, especialmente en S2 (anÃ¡lisis), S5 (diseÃ±o del template) y S6-S8 (construcciÃ³n).

### Agentes paralelos (Task tool)

Claude Code puede lanzar **mÃºltiples agentes en paralelo** usando el Task tool interno. Esto permite analizar varios repos simultÃ¡neamente y luego sintetizar los resultados â€” lo que se llama enfoque **agent-as-a-judge**.

**CuÃ¡ndo usarlo:** S2 â€” Asset Analysis (ver metodologÃ­a especÃ­fica mÃ¡s abajo).

---

## Estructura objetivo del template (borrador S5)

```
gcp-data-agents/cognibi-template/    â† se crea en S6
â”œâ”€â”€ cognibi/
â”‚   â”œâ”€â”€ agent.py                     # Root orchestrator genÃ©rico
â”‚   â”œâ”€â”€ prompts.py                   # Instrucciones base parametrizables
â”‚   â””â”€â”€ sub_agents/
â”‚       â”œâ”€â”€ business_analyst/
â”‚       â”œâ”€â”€ data_engineer/
â”‚       â””â”€â”€ bi_engineer/
â”œâ”€â”€ setup/
â”‚   â”œâ”€â”€ onboard_client.py            # S7: script de onboarding
â”‚   â””â”€â”€ client_config.yaml          # S7: config por cliente
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ schema_template.json        # Formato genÃ©rico de metadata BQ
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ claude-playbook.md          # S8: playbook de prompts
â”œâ”€â”€ demo/                            # S9: demo script
â”œâ”€â”€ CLAUDE.md                        # S8: instrucciones para Claude Code
â””â”€â”€ README.md
```

---

## MetodologÃ­a especial para S1.5: Competitive Landscape

S1.5 valida el posicionamiento estratÃ©gico de CogniBI **antes de construir nada**. Es un checkpoint de estrategia para asegurarse de que hay un hueco real en el mercado antes de invertir en S2â€“S12.

### Preguntas clave a responder
- Â¿Para quÃ© segmento CogniBI gana claramente? Â¿Hay un hueco real no cubierto?
- Â¿CuÃ¡l es el moat real frente a cada competidor?
- Â¿Hay algÃºn red flag que deba cambiar el diseÃ±o del producto antes de S3?

### Competidores a analizar (6)
1. Databricks Genie
2. Looker Conversational Analytics (Google)
3. Power BI Copilot / Tableau Pulse
4. ThoughtSpot / Sigma
5. MicroStrategy Mosaic + Strategy AI Agents
6. DIY: LangChain/LlamaIndex + BigQuery (la alternativa que los tÃ©cnicos intentan en casa)

### Dimensiones de comparaciÃ³n (7)
- **Precio / modelo de negocio**
- **ICP al que va** (enterprise vs midmarket vs startup)
- **Stack tÃ©cnico requerido** (Â¿lock-in a quÃ© plataforma?)
- **Capacidades NL2SQL / viz / contexto de negocio**
- **Deployment** (SaaS vs self-hosted vs managed)
- **Madurez / adoption**
- **Lo que NO hace bien** (gaps que CogniBI puede explotar)

### Paso a paso para ejecutar S1.5

1. **Preguntar a CÃ©sar** quÃ© competidores conoce bien y si hay alguno a priorizar o descartar
2. **Lanzar research en paralelo** â€” WebSearch por competidor (6 bÃºsquedas paralelas), focalizando en pricing, ICP, stack y gaps documentados
3. **Presentar hallazgos a CÃ©sar** antes de sintetizar â€” Â¿refleja lo que Ã©l ve en el mercado?
4. **Generar `01b-competitive-landscape.md`** con tabla comparativa + mapa de posicionamiento + positioning statement refinado + red flags

### Output: `01b-competitive-landscape.md`
- Tabla comparativa 7 dimensiones Ã— 6 competidores + CogniBI
- Mapa de posicionamiento 2Ã—2 (precio vs complejidad tÃ©cnica)
- Positioning statement refinado de CogniBI
- Red flags o ajustes a incorporar antes de S3

---

## MetodologÃ­a especial para S2: Agent-as-a-Judge

S2 no es una sesiÃ³n de anÃ¡lisis lineal. Usa un enfoque multi-agente en paralelo:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ORQUESTADOR (Claude Code principal)             â”‚
â”‚         Experto en arquitecturas agÃ©nticas                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚          â”‚          â”‚          â”‚
       â–¼          â–¼          â–¼          â–¼          â–¼
  [Agente 1]  [Agente 2]  [Agente 3]  [Agente 4]  [Agente 5]
  data-sci-   crm-data-   ca-demos-   ca-api-     /google-adk
  agent       agent       and-tools   quickstarts  skill
  (Explore)   (Explore)   (Explore)   (Explore)   (ADK canon)
       â”‚          â”‚          â”‚          â”‚          â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   SÃNTESIS / JUICIO
              Â¿QuÃ© es reutilizable? Â¿QuÃ©
              va contra ADK best practices?
              Â¿QuÃ© falta para CogniBI?
```

**Paso a paso para ejecutar S2:**

1. **Preguntar a CÃ©sar** quÃ© quiere priorizar / si hay algo que ya conoce bien y no hace falta analizar en profundidad
2. **Lanzar 5 agentes en paralelo** (un Task/Explore por repo + invocar /google-adk para el contexto canÃ³nico de ADK)
3. **Recoger los 5 outputs** y presentar un resumen a CÃ©sar antes de sintetizar
4. **Preguntar a CÃ©sar** si el anÃ¡lisis refleja lo que Ã©l ve / si hay matices a corregir
5. **Generar `02-asset-analysis.md`** con la sÃ­ntesis final + tabla de reutilizaciÃ³n + gaps

**El "juez"** es el orquestador (Claude Code principal) que contrasta los 4 repos entre sÃ­ y contra lo que dice ADK. No es un agente separado â€” es Claude Code usando los resultados de los sub-agentes como evidencia.

**Nota sobre el "agente experto en arquitecturas agÃ©nticas":** por ahora este rol lo cubre Claude Code principal + la skill `/google-adk`. Si en el futuro se quiere un agente especializado persistente, se puede crear un custom agent con instrucciones especÃ­ficas en `cognibi-template/CLAUDE.md` (S8).

---

## Notas por sesiÃ³n

### SesiÃ³n de setup del sprint âœ… (18 Feb 2026)
Todo lo siguiente ocurriÃ³ en la misma sesiÃ³n antes/durante S1:
- Creada estructura `gcp-agents-knowledge-base/cognibi/` con `README.md` y este `CLAUDE.md`
- Creados 10 eventos en Google Calendar (cesar.ramos@lutech-sweeft.es), 15:00â€“17:00, lunes a viernes, semanas del 23 Feb y 2 Mar 2026
- Todos los eventos actualizados con contexto de arranque (prompt exacto + tareas + output esperado)
- Definidas las 3 reglas imperativas de sesiÃ³n (input de CÃ©sar, calendario flexible, guardar foto)
- AÃ±adida documentaciÃ³n de la skill `/google-adk` y la metodologÃ­a agent-as-a-judge para S2
- Evento S2 en Calendar actualizado con el enfoque de 5 agentes en paralelo

### S1 â€” Product Brief âœ… (18 Feb 2026)
- âš ï¸ Ejecutada de manera demasiado autÃ³noma (sin pedir input a CÃ©sar en cada paso) â€” corregido en las reglas
- Explorados `data-science-agent` y `crm-data-agent-cesar` en profundidad via agente Explore
- Definido: problema, deliverable concreto, ICP, modelo de negocio, posicionamiento competitivo
- Output: `gcp-agents-knowledge-base/cognibi/01-product-brief.md`
- Decisiones abiertas pendientes de confirmar con CÃ©sar:
  - Â¿El nombre "CogniBI" es definitivo?
  - Â¿Solo GCP/Gemini o hay que soportar otros LLMs?
  - Â¿UI Streamlit (MVP) o React (enterprise)?

---

## MetodologÃ­a especial para S11: Smoke Test E2E con datos sintÃ©ticos

S11 valida el sistema CogniBI completo simulando un engagement real con un cliente de marketing digital. El objetivo no es solo que el agente funcione â€” es validar que el **workflow completo consultor + Claude Code** (vibe coding) permite construir el modelo, los pipelines y el agente de manera acelerada.

### Dominio del smoke test
- **Fuente de datos:** GA4 export + Google Ads export (esquema estÃ¡ndar BigQuery)
- **Nivel de datos:** mÃ­nimo viable â€” esquema realista + ~100â€“500 filas sintÃ©ticas por tabla clave
- **Pregunta de negocio de ejemplo:** "Â¿CuÃ¡l es el coste por sesiÃ³n por campaÃ±a este mes?" / "Â¿QuÃ© canal convierte mejor?"

### Las 3 fases del smoke test

**Fase 1 â€” Modelo de datos + datos sintÃ©ticos**
Usando Claude Code en modo vibe coding:
- Definir el esquema BigQuery de GA4 + Ads (tablas estÃ¡ndar del export)
- Generar script Python de datos sintÃ©ticos (volumetrÃ­a mÃ­nima viable)
- Cargar en BigQuery (dataset de test)
- Output: `smoke-test/schema/` + `smoke-test/seed_data.py`

**Fase 2 â€” Data pipelines**
Usando Claude Code en modo vibe coding:
- Definir las transformaciones mÃ­nimas necesarias (views o tablas agregadas) para que el agente pueda responder las preguntas de negocio sin SQL complejo
- Implementar como SQL scripts o dbt models ligeros
- Output: `smoke-test/pipelines/`

**Fase 3 â€” Agente CogniBI sobre ese modelo**
Usando el template CogniBI construido en S6â€“S8:
- Onboarding del cliente "demo_ga4_ads" (script S7)
- Configurar `client_config.yaml` con el schema sintÃ©tico
- Ejecutar el agente y hacer 5â€“10 preguntas de negocio reales
- Documentar: Â¿funcionÃ³? Â¿quÃ© fallÃ³? Â¿quÃ© hay que corregir?
- Output: `smoke-test/results.md` con evidencia (queries generadas, charts, errores)

### Dependencias de S11

**S11 no puede ejecutarse sin que estÃ©n completadas:**
- âœ… S6 â€” Build: Template v0.1 (el template genÃ©rico existe y funciona en local)
- âœ… S7 â€” Build: Client Onboarding (el script `onboard_client.py` y `client_config.yaml` estÃ¡n operativos)
- âœ… S8 â€” Build: Claude Code Automation (el playbook de prompts y el CLAUDE.md del template estÃ¡n listos)

**Si alguna de estas sesiones se retrasa**, S11 se mueve automÃ¡ticamente. No tiene sentido hacer el smoke test sobre un template incompleto.

### Lo que S11 valida
- El **template es genÃ©rico** de verdad (no hardcodeado a flights o SFDC)
- El **workflow de onboarding** funciona con un cliente nuevo
- El **vibe coding con Claude Code** es el mÃ©todo de trabajo para la fase de delivery
- Los **gaps reales** del sistema antes de venderlo a un cliente

---

## MetodologÃ­a especial para S12: Governance & Ops Layer

S12 diseÃ±a la capa de operaciones post-entrega del CogniBI Framework. No es solo
documentaciÃ³n â€” es el fundamento que justifica el retainer y hace el producto sostenible.

### Dos capas con audiencias distintas

**Capa 1 â€” Interna (el consultor / CÃ©sar)**

Herramientas y procesos que usa el consultor para operar el agente del cliente:
- **System health:** Cloud Run uptime, latencia p50/p95/p99
- **SQL correctness monitoring:** % queries con retry, errores por tipo, fallback rate
- **Cost tracker:** tokens Gemini + BQ slots por cliente/mes, alertas de budget
- **Incident log:** quÃ© fallÃ³, cuÃ¡ndo, cÃ³mo se resolviÃ³
- **Runbook:** guÃ­a paso a paso para detectar y resolver los fallos mÃ¡s comunes

**Capa 2 â€” Visible al cliente (business value)**

MÃ©tricas que el cliente ve cada mes para demostrar ROI:
- **Time-to-insight:** antes (esperar al analista) vs despuÃ©s (respuesta del agente en segundos)
- **Adoption dashboard:** usuarios activos, queries/dÃ­a, tendencia
- **Query success rate:** % preguntas respondidas satisfactoriamente
- **ROI estimado:** horas de analista ahorradas Ã— coste hora del cliente
- **Report mensual automÃ¡tico** â†’ el entregable que justifica el retainer

### Dependencias de S12

**S12 no puede ejecutarse sin:**
- âœ… S6, S7, S8 completadas (el template y su arquitectura existen)
- âœ… S11 completada (el smoke test ha revelado quÃ© hay que monitorizar en la prÃ¡ctica)

### Output de S12
- `12-governance-ops.md` â€” especificaciÃ³n completa de ambas capas
- IntegraciÃ³n en `cognibi-template/`: estructura de logging y mÃ©tricas a aÃ±adir al template

---

## MetodologÃ­a especial para S13: Market Validation & Battle Cards

S13 evalÃºa el producto CogniBI **terminado** contra la competencia con evidencia real del smoke test (S11) y el sistema de operaciones (S12). Es la validaciÃ³n final antes de salir a vender.

### Objetivo
Confirmar que CogniBI tiene posicionamiento defendible con un producto real funcionando, y que el equipo comercial (CÃ©sar) tiene los argumentarios necesarios para cerrar deals.

### Prerequisito
**S13 no puede ejecutarse sin que estÃ©n completadas:**
- âœ… S6â€“S12 completadas (el producto existe, funciona y tiene capa de ops)
- âœ… S11 completada (el smoke test ha revelado los puntos fuertes y dÃ©biles reales)
- âœ… S1.5 completada (tenemos la lÃ­nea base del anÃ¡lisis competitivo inicial)

### Las 3 fases de S13

**Fase 1 â€” Battle cards por competidor**
Para cada uno de los 6 competidores analizados en S1.5:
- Â¿CuÃ¡ndo gana CogniBI claramente?
- Â¿CuÃ¡ndo pierde (y quÃ© decir en ese caso)?
- CÃ³mo responder las 3 objeciones mÃ¡s comunes de ese competidor
- Evidencia del smoke test que respalda los argumentos

**Fase 2 â€” AnÃ¡lisis de supervivencia (Future-proofing)**
- Â¿CogniBI tiene sentido en 2027 cuando los LLMs sean aÃºn mÃ¡s capaces?
- Â¿CuÃ¡l es el moat a largo plazo? (stack especializado GCP, expertise de implementaciÃ³n, base de clientes, datos propietarios del cliente)
- Escenarios: Â¿quÃ© pasa si Google lanza una versiÃ³n gratuita de Looker Conversational para PyMEs?

**Fase 3 â€” Go-to-market readiness**
- Â¿EstÃ¡ el producto listo para ser vendido? Â¿QuÃ© falta?
- Checklist de materiales de venta: deck, one-pager, demo en vivo, pricing sheet, caso de uso de referencia
- Primeros 3 clientes objetivo: perfil, canal de entrada, propuesta de valor especÃ­fica

### Output: `13-market-validation.md`
- Battle cards (1 por competidor, formato estandarizado)
- SecciÃ³n "Future-proofing": escenarios y moat analysis
- Checklist go-to-market con estado de cada Ã­tem
- Resumen ejecutivo: Â¿CogniBI estÃ¡ listo para vender? Â¿QuÃ© hacer primero?

---

## Reglas de trabajo para Claude Code

- DocumentaciÃ³n â†’ esta carpeta (`gcp-agents-knowledge-base/cognibi/`)
- CÃ³digo del template â†’ `gcp-data-agents/cognibi-template/` (se crea en S6)
- **Al terminar cada sesiÃ³n:** actualizar este `CLAUDE.md` (tabla de estado + notas) y el `README.md` â€” SIEMPRE, sin excepciÃ³n
- **Antes de cada tarea:** usar `AskUserQuestion` para confirmar con CÃ©sar â€” SIEMPRE
- **Al modificar el plan de trabajo (aÃ±adir, mover, fusionar o eliminar sesiones):** (1) consultar primero el Google Calendar de CÃ©sar (cesar.ramos@lutech-sweeft.es) para ver las fechas reales de todos los eventos CogniBI existentes, (2) calcular el nuevo orden respetando siempre un dÃ­a de gap entre sesiones y saltando fines de semana a la siguiente semana, (3) actualizar o crear los eventos de Calendar consecuentemente. SIEMPRE, sin excepciÃ³n. No asumir fechas sin consultar el calendario real.
- Stack fijo: Google ADK + Gemini + BigQuery + GCP. No proponer alternativas salvo que CÃ©sar lo indique.
- Idioma de documentaciÃ³n: espaÃ±ol. CÃ³digo: inglÃ©s.
